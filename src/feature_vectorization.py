# -*- coding: utf-8 -*-
"""feature_vectorization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17mNCn9vLRRqa3tKEdkxys5mOfSTFI9vb
"""

pip3 install tqdm

import os
import json
import tqdm
import numpy as np
import pandas as pd
import lightgbm as lgb
import multiprocessing
from src.features import PEFeatureExtractor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (roc_auc_score, make_scorer)

class FeatureVectorization:
  def raw_feature_iterator(file_paths):
      """
      Yield raw feature strings from the inputed file paths
      """
      for path in file_paths:
          with open(path, "r") as fin:
              for line in fin:
                  yield line


  def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):
      """
      Vectorize a single sample of raw features and write to a large numpy file
      """
      raw_features = json.loads(raw_features_string)
      feature_vector = extractor.process_raw_features(raw_features)

      y = np.memmap(y_path, dtype=np.float32, mode="r+", shape=nrows)
      y[irow] = raw_features["label"]

      X = np.memmap(X_path, dtype=np.float32, mode="r+", shape=(nrows, extractor.dim))
      X[irow] = feature_vector


  def vectorize_unpack(args):
      """
      Pass through function for unpacking vectorize arguments
      """
      return vectorize(*args)


  def vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows):
      """
      Vectorize a subset of data and write it to disk
      """
      # Create space on disk to write features to
      X = np.memmap(X_path, dtype=np.float32, mode="w+", shape=(nrows, extractor.dim))
      y = np.memmap(y_path, dtype=np.float32, mode="w+", shape=nrows)
      del X, y

      # Distribute the vectorization work
      pool = multiprocessing.Pool()
      argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)
                          for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths)))
      for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):
          pass


  def create_vectorized_features(data_dir, feature_version=2):
      """
      Create feature vectors from raw features and write them to disk
      """
      extractor = PEFeatureExtractor(feature_version)

      print("Vectorizing training set")
      X_path = os.path.join(data_dir, "X_train.dat")
      y_path = os.path.join(data_dir, "y_train.dat")
      raw_feature_paths = [os.path.join(data_dir, "train_features_{}.jsonl".format(i)) for i in range(6)]
      nrows = sum([1 for fp in raw_feature_paths for line in open(fp)])
      vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows)

      print("Vectorizing test set")
      X_path = os.path.join(data_dir, "X_test.dat")
      y_path = os.path.join(data_dir, "y_test.dat")
      raw_feature_paths = [os.path.join(data_dir, "test_features.jsonl")]
      nrows = sum([1 for fp in raw_feature_paths for line in open(fp)])
      vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows)