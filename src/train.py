# Import Packages
import tensorflow as tf
import ember
import numpy as np
import matplotlib.pyplot as plt
import time
import json
import datetime
import argparse
import random
import os
import lightgbm as lgb

from pathlib import Path
#from src.feature_vectorization import *
#from src.feature_vectorization import X_train
import src.feature_vectorization
from src.feature_vectorization import *
from src.ModelClass import *
from tensorflow import keras
from tensorflow.keras import layers
from src.features import *


def build_model():
    model = My_Model()
    print("Reaching build_model. This is where the model will be built/imported.")
    return model


def build_dataset():
    train_dataset = "/content/gdrive/MyDrive/COML Final/ember2018/"
    train_dataset_fp = tf.keras.utils.get_file(
        fname=train_dataset, origin='https://ember.elastic.co/ember_dataset_2018_2.tar.bz2')

    features = [ByteHistogram(),
                ByteEntropyHistogram(),
                StringExtractor(),
                GeneralFileInfo(),
                HeaderFileInfo(),
                SectionInfo(),
                ImportsInfo(),
                ExportsInfo()]

    print("Local copy of the dataset file: {}".format(train_dataset_fp))

    # column order in CSV file
    column_names = ['column', 'sha256',
                    'appeared', 'label', 'avclass', 'subset']
    feature_names = column_names[:-1]
    label_name = column_names[3]

    print("Features: {}".format(feature_names))
    print("Label: {}".format(label_name))

    batch_size = 32
    train_dataset = tf.data.experimental.make_csv_dataset(
        train_dataset_fp,
        batch_size,
        column_names=column_names,
        label_name=label_name,
        num_epochs=1)
    features, labels = next(iter(train_dataset))
    print(features)
    print("Reaching build_dataset. Since our dataset is built outside of this file, we will access the built one from there.")
    return train_dataset


def loss(model, x, y, training):
    # training=training is needed only if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    y_ = model(x, training=training)
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True)

    return loss_object(y_true=y, y_pred=y_)


def grad(model, inputs, targets):
    with tf.GradientTape() as tape:
        loss_value = loss(model, inputs, targets, training=True)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)


def main():
    model = build_model()
    #X_train, y_train, X_test, y_test = vectorization('/content/gdrive/MyDrive/COML Final/ember2018/')
    X_train, y_train, X_test, y_test = ember.read_vectorized_features(
        "./ember2018/")
    metadata_dataframe = ember.read_metadata("./ember2018/")
    lgbm_model = ember.train_model("./ember2018/")
    train_dataset = build_dataset()
    #loss_object = loss(model, X_train, y_train, training=train_dataset)

    features, labels = next(iter(train_dataset))
    l = loss(model, features, labels, training=True)

    # lgbm_model = ember.train_model("./ember2018/")

    print("Loss test: {}".format(l))

    # Set up optimizer
    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

    # Calculate a single optimization step
    loss_value, grads = grad(model, features, labels)

    print("Step: {}, Initial Loss: {}".format(
        optimizer.iterations.numpy(), loss_value.numpy()))

    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    print("Step: {}, Loss: {}".format(optimizer.iterations.numpy(),
          loss(model, features, labels, training=True).numpy()))

    # Train Model

    # Note: Rerunning this cell uses the same model variables

    # Keep results for plotting
    train_loss_results = []
    train_accuracy_results = []

    num_epochs = 1

    for epoch in range(num_epochs):
        epoch_loss_avg = tf.keras.metrics.Mean()
        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

    # Training loop - using batches of 32
    for x, y in train_dataset:
        # Optimize the model
        loss_value, grads = grad(model, X_train, y_train)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Track progress
        epoch_loss_avg.update_state(loss_value)  # Add current batch loss
        # Compare predicted label to actual label
        # training=True is needed only if there are layers with different
        # behavior during training versus inference (e.g. Dropout).
        epoch_accuracy.update_state(y, model(x, training=True))

    # End epoch
    train_loss_results.append(epoch_loss_avg.result())
    train_accuracy_results.append(epoch_accuracy.result())

    if epoch % 50 == 0:
        print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(
            epoch, epoch_loss_avg.result(), epoch_accuracy.result()))

    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
    fig.suptitle('Training Metrics')

    axes[0].set_ylabel("Loss", fontsize=14)
    axes[0].plot(train_loss_results)

    axes[1].set_ylabel("Accuracy", fontsize=14)
    axes[1].set_xlabel("Epoch", fontsize=14)
    axes[1].plot(train_accuracy_results)
    plt.show()


main()
