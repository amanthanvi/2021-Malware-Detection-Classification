# -*- coding: utf-8 -*-
"""Data Download, Extraction, and Validation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qYZwCJovLyETv_AS74cTNUfogiG4c3Qb

Imports
"""

!pip install opendatasets
!pip install git+https://github.com/elastic/ember.git

import opendatasets as od
import tarfile
import ember
import os
import lightgbm as lgb
import altair as alt

"""Data Download"""

od.download("https://ember.elastic.co/ember_dataset_2018_2.tar.bz2") #imports dataset

"""Data Extraction"""

tar = tarfile.open("/content/ember_dataset_2018_2.tar.bz2", "r:bz2")  
tar.extractall() #unzips and decompresses dataset
tar.close()

"""Data Validation"""

data_dir = "/content/ember2018" #instantiates data directory

ember.create_vectorized_features(data_dir, 1)
_ = ember.create_metadata(data_dir)

emberdf = ember.read_metadata(data_dir)
X_train, y_train, X_test, y_test = ember.read_vectorized_features(data_dir, feature_version=1)
lgbm_model = lgb.Booster(model_file=os.path.join(data_dir, "ember_model_2018.txt"))

plotdf = emberdf.copy()
gbdf = plotdf.groupby(["label", "subset"]).count().reset_index()
alt.Chart(gbdf).mark_bar().encode( #creates data validation graph
    alt.X('subset:O', axis=alt.Axis(title='Subset')),
    alt.Y('sum(sha256):Q', axis=alt.Axis(title='Number of samples')),
    alt.Color('label:N', scale=alt.Scale(range=["#00b300", "#3333ff", "#ff3333"]), legend=alt.Legend(values=["unlabeled", "benign", "malicious"]))
)

"""Convert to CSV File"""

class CSV:

  def read_metadata_record(raw_features_string):

    all_data = json.loads(raw_features_string)

    metadata_keys = {"sha256", "appeared", "label", "avclass"}
    
    return {k: all_data[k] for k in all_data.keys() & metadata_keys}
  
  def create_metadata(data_dir):
  
    pool = multiprocessing.Pool()

    train_feature_paths = [os.path.join(data_dir, "train_features_{}.jsonl".format(i)) for i in range(6)]
    train_records = list(pool.imap(read_metadata_record, raw_feature_iterator(train_feature_paths)))

    metadata_keys = ["sha256", "appeared", "label", "avclass"]
    ordered_metadata_keys = [k for k in metadata_keys if k in train_records[0].keys()]

    train_metadf = pd.DataFrame(train_records)[ordered_metadata_keys]
    train_metadf.to_csv(os.path.join(data_dir, "train_metadata.csv"))

    train_records = [dict(record, **{"subset": "train"}) for record in train_records]

    test_feature_paths = [os.path.join(data_dir, "test_features.jsonl")]
    test_records = list(pool.imap(read_metadata_record, raw_feature_iterator(test_feature_paths)))

    test_metadf = pd.DataFrame(test_records)[ordered_metadata_keys]
    test_metadf.to_csv(os.path.join(data_dir, "test_metadata.csv"))

    test_records = [dict(record, **{"subset": "test"}) for record in test_records]

    all_metadata_keys = ordered_metadata_keys + ["subset"]
    metadf = pd.DataFrame(train_records + test_records)[all_metadata_keys]
    metadf.to_csv(os.path.join(data_dir, "metadata.csv"))
    return metadf

  def read_metadata(data_dir):
  
    return pd.read_csv(os.path.join(data_dir, "metadata.csv"), index_col=0)